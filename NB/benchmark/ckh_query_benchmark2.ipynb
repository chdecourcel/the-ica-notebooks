{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://clickhouse-driver.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from uuid import uuid4\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.core.display import Markdown\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "from clickhouse_driver import Client\n",
    "\n",
    "import rafal\n",
    "\n",
    "# Config loading : modify in the config.json file\n",
    "config = json.load(open('config.json'))\n",
    "user= config['Rafal.user']\n",
    "url = config['Rafal.url']\n",
    "print(url)\n",
    "\n",
    "pwd_exists = bool(config.get('Rafal.password', None)) \n",
    "wPwd= widgets.Password(value= '',\n",
    "                      placeholder= 'from config' if pwd_exists else 'Enter password',\n",
    "                      disabled= pwd_exists\n",
    "                     )\n",
    "\n",
    "display(widgets.HBox([widgets.Label('Password for Rafal API :'), wPwd]))\n",
    "\n",
    "# proxy parameters\n",
    "proxies = (config['proxies'] if config['proxies']['http'] or config['proxies']['https'] \n",
    "           else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import pytz\n",
    "tz= pytz.timezone('Europe/Paris')\n",
    "tst= tz.localize(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk_to_dataframe(ckh_result):\n",
    "    \"\"\"Convert a clickhouse query result into Dataframe\"\"\"\n",
    "    return pd.DataFrame.from_records(ckh_result[0], columns= [col for col, _ in ckh_result[1]])\n",
    "\n",
    "def benchmarkQueries(client, queryDict, nb= 5, verbose= True):\n",
    "    \"\"\"compute perf stats on a dictionary of ClickHouse Queries\n",
    "        queryDict: dict of name: query\n",
    "            name = label name of the quey (any string)\n",
    "            query = clickhouse query sent to the clickhouse driver\n",
    "        nb: number of iteration to run the benchmark (default = 5)\n",
    "        \n",
    "        return: \n",
    "            tuple of (stats_dataframe, list_of_queries_result)\n",
    "    \"\"\"\n",
    "    stats=[]\n",
    "    \n",
    "    for i in range(nb):\n",
    "        dfs= []\n",
    "        for name, query in queryDict.items():\n",
    "            dfs.append(client.execute(query, with_column_types= True, settings= settings))\n",
    "            dd= dict(qname=name, i=i, elapsed= client.last_query.elapsed, **client.last_query.profile_info.__dict__)\n",
    "            if verbose:\n",
    "                print(dd)\n",
    "            else:\n",
    "                print('.', end='')\n",
    "            stats.append(dd)\n",
    "    client.disconnect()\n",
    "    return pd.DataFrame.from_records(stats), dfs\n",
    "\n",
    "def plot_duration(stats, title= None, ax= None):\n",
    "    \"\"\"Draw a barplot to show average query duration\"\"\"\n",
    "    nb= len(stats.i.unique())\n",
    "    g = sns.barplot(x=\"qname\", y=\"elapsed\", data= stats, palette=\"muted\", ax= ax)\n",
    "    g.set_title(title + f'\\n(median on {nb} iterations)')\n",
    "    g.set_ylabel(\"duration in sec (avg)\")\n",
    "    g.set_xlabel(\"\")\n",
    "    sns.despine(left=True)\n",
    "    return g\n",
    "\n",
    "def getQueryStats(client_name= 'ClickHouse python-driver', os_user= None, fromDateTime= None, query= None, limit= 300):\n",
    "    \"\"\" request query stats from table [distributed_query_log]\n",
    "    \"\"\"\n",
    "    wTst = (f\"and query_start_time >= '{fromDateTime.astimezone(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')}'\" \n",
    "            if fromDateTime else '')\n",
    "    wOs = f\"and os_user= '{os_user}'\" if os_user else ''\n",
    "    query_mon= f\"\"\"select `type`, `event_date`, `event_time`, `query_start_time`, `query_duration_ms`, `read_rows`, `read_bytes`, `written_rows`, `written_bytes`, \n",
    "        `result_rows`, `result_bytes`, `memory_usage`,`query`, `is_initial_query`,`user`, `query_id` ,\n",
    "        `os_user`, `client_hostname`, `Settings.Names`, `Settings.Values`\n",
    "    from distributed_query_log dql \n",
    "    where client_name = '{client_name}' {wOs} {wTst} and type != 'QueryStart'\n",
    "    order by event_time desc limit {limit}\"\"\"\n",
    "    res= client.execute(query_mon, with_column_types= True, settings= settings)\n",
    "    df= chk_to_dataframe(res)\n",
    "    if query:\n",
    "        df= df[df['query'] == query]\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host, port= url.replace('http://', '').split(':')\n",
    "host, port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {'max_block_size': 100000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(host= host, port= 9000, password= 'thisIsADevPassword', settings= settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.execute('SHOW DATABASES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res= client.execute('SHOW CREATE TABLE mtms')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where1= \"\"\"((scenario GLOBAL IN \n",
    "    (\n",
    "        SELECT DISTINCT scenario\n",
    "        FROM default.diffusions\n",
    "        WHERE ((diffusions[\n",
    "        (\n",
    "            SELECT indexOf(dates, toDate('2047-06-05'))\n",
    "            FROM datearray\n",
    "        )]) < 0.34) AND (SensId = 2)\n",
    "    )) AND ((trade >= 0.) AND (trade <= 950.)) AND (toUInt64(csaId) IN \n",
    "    (\n",
    "        SELECT csaId\n",
    "        FROM dict.csaId\n",
    "        WHERE csa IN ('BANK01')\n",
    "    )) AND (Date IN ('2019-04-16'))) AND ((Date = '2019-04-16') AND (commit IN (1)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where2= \"\"\"((trade >= 0.) AND (trade <= 950.)) AND (commit IN (1))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ref = \"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "SELECT    mtm, dates, scenario\n",
    "FROM (\n",
    "    SELECT    mtm, mtms_mtms_datearray_dates, scenario\n",
    "    FROM\n",
    "        (\n",
    "        SELECT\n",
    "            scenario,\n",
    "            {func}ForEach(arrayMap(x -> x * factor, mtms)) AS mtm\n",
    "        FROM\n",
    "            mtms\n",
    "        WHERE {where}\n",
    "        GROUP BY scenario\n",
    "        ) ARRAY JOIN mtm,\n",
    "        arrayEnumerate(mtm) as mtms_mtms_datearray_dates ) ALL\n",
    "INNER JOIN (\n",
    "    SELECT\n",
    "        dates,    mtms_mtms_datearray_dates\n",
    "    FROM\n",
    "        datearray ARRAY JOIN arrayEnumerate(dates) as mtms_mtms_datearray_dates,\n",
    "        dates )\n",
    "        USING mtms_mtms_datearray_dates\n",
    ") WHERE {WhereArray}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_new= \"\"\"\n",
    "SELECT   {func}Merge(MtmsAggreg) AS mtm,   fdate,   scenario\n",
    "FROM\n",
    "(\n",
    "   SELECT       MtmsAggregs,       dates AS fdates,       scenario\n",
    "   FROM\n",
    "   (\n",
    "       SELECT           scenario,           Date,\n",
    "           {func}StateForEach(arrayMap(x -> (x * factor), mtms)) AS MtmsAggregs\n",
    "       FROM mtms\n",
    "       WHERE {where}\n",
    "       GROUP BY\n",
    "           scenario,\n",
    "           Date\n",
    "   )\n",
    "   INNER JOIN\n",
    "   (\n",
    "       SELECT dates, Date FROM datearraywithdate\n",
    "   ) USING (Date)\n",
    ")\n",
    "ARRAY JOIN\n",
    "   arrayResize(fdates, length(MtmsAggregs)) AS fdate,\n",
    "   MtmsAggregs AS MtmsAggreg\n",
    "WHERE {WhereArray}\n",
    "GROUP BY\n",
    "   fdate,\n",
    "   scenario\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_new2= \"\"\"\n",
    "SELECT   sum(MtmsAggreg) AS mtm,   fdate,   scenario\n",
    "FROM\n",
    "(\n",
    "   SELECT       MtmsAggregs,       dates AS fdates,       scenario\n",
    "   FROM\n",
    "   (\n",
    "       SELECT           scenario,           Date,\n",
    "           sumForEach(arrayMap(x -> (x * factor), mtms)) AS MtmsAggregs\n",
    "       FROM mtms\n",
    "       WHERE {where}\n",
    "       GROUP BY\n",
    "           scenario,\n",
    "           Date\n",
    "   )\n",
    "   INNER JOIN\n",
    "   (\n",
    "       SELECT dates, Date FROM datearraywithdate\n",
    "   ) USING (Date)\n",
    ")\n",
    "ARRAY JOIN\n",
    "   arrayResize(fdates, length(MtmsAggregs)) AS fdate,\n",
    "   MtmsAggregs AS MtmsAggreg\n",
    "WHERE {WhereArray}\n",
    "GROUP BY\n",
    "   fdate,\n",
    "   scenario\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query_new1= \"\"\"\n",
    "SELECT \n",
    "    sum(MtmAggreg) as mtm,     fdate,    scenario \n",
    "FROM \n",
    "(\n",
    "    SELECT         scenario,\n",
    "        sumForEach(arrayMap(x -> (x * factor), mtms)) AS MtmsAggreg,\n",
    "        arrayResize(any(fdates), length(MtmsAggreg)) as fdatesAggr\n",
    "    FROM mtms\n",
    "    WHERE {where}\n",
    "    GROUP BY \n",
    "        scenario\n",
    ")\n",
    "ARRAY JOIN \n",
    "    MtmsAggreg AS MtmAggreg,\n",
    "    fdatesAggr AS fdate\n",
    "GROUP BY fdate, scenario \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "query_new2= \"\"\"\n",
    "SELECT \n",
    "    sum(MtmAggreg) as mtm,     fdate,    scenario \n",
    "FROM \n",
    "(\n",
    "    SELECT         scenario,        \n",
    "        sumForEach(arrayMap(x -> (x * factor), mtms)) AS MtmsAggreg,\n",
    "        fdates\n",
    "    FROM mtms\n",
    "    WHERE {where}\n",
    "    GROUP BY \n",
    "        scenario, fdates\n",
    ")\n",
    "ARRAY JOIN \n",
    "    MtmsAggreg AS MtmAggreg, \n",
    "    arrayResize(fdates, length(MtmsAggreg)) AS fdate\n",
    "GROUP BY fdate, scenario \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb of iterations\n",
    "nb= 5\n",
    "# where clause\n",
    "wheres= dict(narrow= where1, large= where2)\n",
    "# metric function\n",
    "funcs= ['sum', 'count', 'avg']\n",
    "\n",
    "# where clause & nb of iterations\n",
    "#filters= dict(narrow= (where1, 2), large= (where2, 2))\n",
    "WhereArray= \"1=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs= [{'func': func,\n",
    "        'where': where,\n",
    "        'name': f\"{func} agg\\n{name} where\",\n",
    "        'queries': dict(query_ref= query_ref.format(where= where, func= func, WhereArray= WhereArray),\n",
    "                        query_new= query_new.format(where= where, func= func, WhereArray= WhereArray),\n",
    "                       ),\n",
    "        'iterations': nb}\n",
    "       for func in funcs for name, where in wheres.items()]\n",
    "       #for name, (where, nb) in filters.items()]\n",
    "\n",
    "for run in runs:\n",
    "    if run['func'] == 'sum':\n",
    "        run['queries']['query_sum']= query_new2.format(where= run['where'], WhereArray= WhereArray)\n",
    "        #print(run['queries']['query_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats0= {}\n",
    "dfs= {}\n",
    "g= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggDict= dict(duration_sec_avg= ('elapsed', 'median'), \n",
    "              rows= ('rows', 'median'), \n",
    "              blocks= ('blocks', 'median'), \n",
    "              bytes= ('bytes', 'median'),\n",
    "              nb= ('elapsed', 'count'),\n",
    "             )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, axes= plt.subplots(1,2, sharey= True)\n",
    "fig.suptitle(f'benchmark results', fontsize= 16)\n",
    "fig.set_size_inches(13, 6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for run, ax in zip(runs, axes):\n",
    "for run in runs:\n",
    "    name= run['name']\n",
    "    print(f\"\\n--> Running serie '{name}' with {run['iterations']} iterations :\")\n",
    "    stats0[name], dfs[name]= benchmarkQueries(client, \n",
    "                                              queryDict= run['queries'], \n",
    "                                              nb= run['iterations'], \n",
    "                                              verbose= False)\n",
    "    #g[name]= plot_duration(stats0[name], title= f\"{name} filter\", ax= ax)\n",
    "    print('\\n', f\"{name} filter\", 'with WHERE clause :\\n', run['where'])\n",
    "\n",
    "stats= pd.concat(stats0, names=['serie'])\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed query log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_name= 'ClickHouse python-driver'\n",
    "client_hostname= 'LAPTOP-M7C54VOV'\n",
    "os_user= 'chdec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# request query stats from table [distributed_query_log], filtered by previous queries\n",
    "dropFields= ['query', 'is_initial_query', 'client_hostname', 'query_start_time']\n",
    "distQueryLog= pd.concat({run['name']: pd.concat({name: getQueryStats(fromDateTime= tst, query= query) \n",
    "                                                 for name, query in run['queries'].items()}, \n",
    "                                                names= ['qname']) \n",
    "                         for run in runs}, names= ['serie']\n",
    "                     ).drop(dropFields, axis= 1)\n",
    "distQueryLog.index.names = ['serie', 'qname', 'i']\n",
    "\n",
    "# concat results with clickHouse client stats\n",
    "ckhqStats= pd.concat([stats.reset_index(level=1, drop= True).set_index(['qname', 'i'], append= True), \n",
    "                      distQueryLog], axis= 1\n",
    "                    ).assign(query_duration_sec= lambda x: x.query_duration_ms/1000)\n",
    "\n",
    "ckhqStats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot a seaborn bar graph, with average stats\n",
    "fig1, axes = plt.subplots(2,2)\n",
    "fig1.set_size_inches(16, 14)\n",
    "data= ckhqStats.reset_index()\n",
    "flds= [\"elapsed\", \"query_duration_sec\", \"memory_usage\", \"read_bytes\"]\n",
    "for field, ax in zip(flds, axes.flatten()):\n",
    "    g = sns.barplot(x=\"serie\", y= field, hue= \"qname\",data= data, palette=\"muted\", ax= ax)\n",
    "    g.set_title(field)\n",
    "    g.set_ylabel(f\"{field} (avg)\")\n",
    "    g.set_xlabel(\"\")\n",
    "    g.set_xticklabels(g.get_xticklabels(), \n",
    "                      rotation= 35, \n",
    "                      horizontalalignment='right',\n",
    "                      fontweight='light',\n",
    "                      fontsize='large')\n",
    "\n",
    "plt.suptitle(f\"Benchmark on {url}\\n{nb} iterations for each, {tst:at %Y-%m-%d %H:%M [UTC %z]}\", \n",
    "             fontsize= 'x-large', y=1.03)\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggDict2= dict(elapsed= 'median', query_duration_sec= 'median', read_bytes= 'median', result_rows='median',\n",
    "               memory_usage= 'median', event_time= 'first', query_id= 'first')\n",
    "queryReport= ckhqStats.groupby(['serie', 'qname']\n",
    "                                 ).agg(aggDict2).unstack(0)\n",
    "delta = queryReport._get_numeric_data().apply(lambda df: df.loc['query_new'] / df.loc['query_ref'] -1)\n",
    "fmt_cols= {col: \"{:,}\"  for col, typ in queryReport.dtypes.iteritems() if typ == 'int64'}\n",
    "fmt_cols.update({('memory_usage', 'large'): \"{:,.0f}\", ('memory_usage', 'narrow'): \"{:,.0f}\"})\n",
    "queryReport.style.format(fmt_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache effet ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % increase of first query duration compared to median duration\n",
    "cacheEffect= ckhqStats.xs(0, level='i').query_duration_sec - ckhqStats.groupby(level=[0,1]).query_duration_sec.median()\n",
    "cacheEffect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max cache value in sec\n",
    "cacheEffect.index[cacheEffect.argmax()], cacheEffect.iloc[cacheEffect.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckhqStats.loc[('sum agg\\nnarrow where')].query_duration_sec#('query_ref', level='qname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
